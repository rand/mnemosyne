#!/usr/bin/env python3
"""Semantic evaluation metrics using LLM-as-a-Judge.

Implements principled semantic similarity evaluation for DSPy modules,
replacing exact string matching with LLM-based judgment that captures
semantic equivalence, detail level, and quality dimensions.

Based on DSPy best practices and research on LLM evaluation:
- https://dspy.ai/
- https://arxiv.org/abs/2412.15298 (DSPy Teleprompter Evaluation)
- https://medium.com/@mayankkhulbe1903/llm-as-a-judge-evaluating-ai-agents-with-dspy-7223f0c76bcd
"""

import dspy
from typing import List, Tuple
import logging

logger = logging.getLogger(__name__)


# =============================================================================
# Semantic Similarity Judge
# =============================================================================

class RequirementSimilarityJudge(dspy.Signature):
    """Evaluate semantic similarity between two software requirements.

    Judge whether a predicted requirement captures the same intent and scope
    as a gold standard requirement, accounting for differences in wording,
    detail level, and specificity.

    Scoring criteria:
    - 1.0: Semantically equivalent (same intent, similar scope)
    - 0.7-0.9: Captures core intent but different detail level or specificity
    - 0.4-0.6: Partially overlapping (related but different scope)
    - 0.1-0.3: Tangentially related (same domain but different focus)
    - 0.0: Unrelated or contradictory
    """

    gold_requirement = dspy.InputField(
        desc="Reference requirement from training data"
    )
    predicted_requirement = dspy.InputField(
        desc="Requirement generated by the model"
    )

    reasoning = dspy.OutputField(
        prefix="Reasoning: Let's analyze step by step:",
        desc="Detailed comparison of semantic meaning, scope, and specificity"
    )
    semantic_match = dspy.OutputField(
        desc="Whether requirements capture the same intent (yes/no/partial)"
    )
    similarity_score = dspy.OutputField(
        desc="Numeric similarity score from 0.0 to 1.0"
    )


class SemanticSimilarityJudge(dspy.Module):
    """DSPy module for judging semantic similarity between requirements."""

    def __init__(self):
        super().__init__()
        self.judge = dspy.ChainOfThought(RequirementSimilarityJudge)

    def forward(self, gold: str, pred: str) -> float:
        """Evaluate similarity between gold and predicted requirement.

        Args:
            gold: Reference requirement from training data
            pred: Predicted requirement from model

        Returns:
            Similarity score from 0.0 to 1.0
        """
        result = self.judge(
            gold_requirement=gold,
            predicted_requirement=pred
        )

        try:
            # Parse score from string output
            score = float(result.similarity_score)
            # Clamp to valid range
            return max(0.0, min(1.0, score))
        except (ValueError, AttributeError) as e:
            logger.warning(f"Failed to parse similarity score: {e}")
            # Fallback: use semantic_match as rough estimate
            match = str(result.semantic_match).lower()
            if "yes" in match:
                return 0.9
            elif "partial" in match:
                return 0.5
            else:
                return 0.1


# =============================================================================
# Bipartite Matching for Requirement Sets
# =============================================================================

def find_best_matches(
    gold_reqs: List[str],
    pred_reqs: List[str],
    judge: SemanticSimilarityJudge,
    threshold: float = 0.4
) -> List[Tuple[int, int, float]]:
    """Find best semantic matches between gold and predicted requirements.

    Uses greedy bipartite matching to pair requirements based on semantic
    similarity, ensuring each requirement is matched at most once.

    Args:
        gold_reqs: Reference requirements
        pred_reqs: Predicted requirements
        judge: Semantic similarity judge
        threshold: Minimum similarity for a valid match

    Returns:
        List of (gold_idx, pred_idx, score) tuples for valid matches
    """
    # Compute all pairwise similarities
    similarities = []
    for g_idx, gold in enumerate(gold_reqs):
        for p_idx, pred in enumerate(pred_reqs):
            score = judge(gold, pred)
            if score >= threshold:
                similarities.append((g_idx, p_idx, score))

    # Sort by score (descending)
    similarities.sort(key=lambda x: x[2], reverse=True)

    # Greedy matching: take best scores that don't reuse indices
    matches = []
    used_gold = set()
    used_pred = set()

    for g_idx, p_idx, score in similarities:
        if g_idx not in used_gold and p_idx not in used_pred:
            matches.append((g_idx, p_idx, score))
            used_gold.add(g_idx)
            used_pred.add(p_idx)

    return matches


# =============================================================================
# Semantic F1 Metrics
# =============================================================================

def semantic_requirement_f1(
    example: dspy.Example,
    pred: dspy.Prediction,
    trace=None,
    threshold: float = 0.7
) -> float:
    """Compute semantic F1 score for requirement extraction.

    Uses LLM-as-a-Judge to evaluate semantic similarity between predicted
    and gold requirements, then computes precision/recall/F1 based on
    best bipartite matching.

    Args:
        example: Training example with gold requirements
        pred: Model prediction with predicted requirements
        trace: Optional DSPy trace (unused)
        threshold: Minimum similarity score to count as a match

    Returns:
        F1 score from 0.0 to 1.0
    """
    try:
        gold_reqs = list(example.requirements)
        pred_reqs = list(pred.requirements) if hasattr(pred, 'requirements') else []

        if not pred_reqs:
            return 0.0

        if not gold_reqs:
            # Edge case: if gold is empty but pred is not, score is 0
            return 0.0

        # Initialize judge
        judge = SemanticSimilarityJudge()

        # Find best matches
        matches = find_best_matches(gold_reqs, pred_reqs, judge, threshold)

        # Compute precision and recall based on matches
        # Precision: how many predicted requirements match gold requirements
        precision = len(matches) / len(pred_reqs)

        # Recall: how many gold requirements are covered by predictions
        recall = len(matches) / len(gold_reqs)

        # F1 score
        if precision + recall == 0:
            return 0.0

        f1 = 2 * (precision * recall) / (precision + recall)

        logger.debug(
            f"Semantic F1: {f1:.3f} (P={precision:.3f}, R={recall:.3f}, "
            f"{len(matches)}/{len(pred_reqs)} matches)"
        )

        return f1

    except Exception as e:
        logger.error(f"Error computing semantic F1: {e}")
        return 0.0


def semantic_requirement_f1_weighted(
    example: dspy.Example,
    pred: dspy.Prediction,
    trace=None,
    threshold: float = 0.7
) -> float:
    """Compute weighted semantic F1 using similarity scores.

    Similar to semantic_requirement_f1 but uses actual similarity scores
    rather than binary matches, giving partial credit for imperfect matches.

    Args:
        example: Training example with gold requirements
        pred: Model prediction with predicted requirements
        trace: Optional DSPy trace (unused)
        threshold: Minimum similarity score to include in matching

    Returns:
        Weighted F1 score from 0.0 to 1.0
    """
    try:
        gold_reqs = list(example.requirements)
        pred_reqs = list(pred.requirements) if hasattr(pred, 'requirements') else []

        if not pred_reqs or not gold_reqs:
            return 0.0

        # Initialize judge
        judge = SemanticSimilarityJudge()

        # Find best matches
        matches = find_best_matches(gold_reqs, pred_reqs, judge, threshold)

        # Weighted precision: average similarity of matched predictions
        if matches:
            pred_scores = [score for _, _, score in matches]
            # Add 0 for unmatched predictions
            pred_scores.extend([0.0] * (len(pred_reqs) - len(matches)))
            precision = sum(pred_scores) / len(pred_reqs)
        else:
            precision = 0.0

        # Weighted recall: average similarity of matched gold requirements
        if matches:
            gold_scores = [score for _, _, score in matches]
            # Add 0 for unmatched gold requirements
            gold_scores.extend([0.0] * (len(gold_reqs) - len(matches)))
            recall = sum(gold_scores) / len(gold_reqs)
        else:
            recall = 0.0

        # F1 score
        if precision + recall == 0:
            return 0.0

        f1 = 2 * (precision * recall) / (precision + recall)

        logger.debug(
            f"Weighted Semantic F1: {f1:.3f} (P={precision:.3f}, R={recall:.3f})"
        )

        return f1

    except Exception as e:
        logger.error(f"Error computing weighted semantic F1: {e}")
        return 0.0


# =============================================================================
# Boolean Validation Metrics
# =============================================================================

def semantic_boolean_match(
    example: dspy.Example,
    pred: dspy.Prediction,
    field_name: str,
    trace=None
) -> float:
    """Evaluate binary field match with semantic parsing.

    Handles various boolean representations (True/False, "true"/"false",
    "yes"/"no") and returns 1.0 for match, 0.0 for mismatch.

    Args:
        example: Training example with gold boolean value
        pred: Model prediction with predicted boolean
        field_name: Name of the boolean field to compare
        trace: Optional DSPy trace (unused)

    Returns:
        1.0 if match, 0.0 if mismatch
    """
    try:
        gold_value = getattr(example, field_name, None)
        pred_value = getattr(pred, field_name, None)

        if gold_value is None or pred_value is None:
            return 0.0

        # Normalize to boolean
        def to_bool(val):
            if isinstance(val, bool):
                return val
            if isinstance(val, str):
                return val.strip().lower() in ['true', 'yes', '1']
            return bool(val)

        return 1.0 if to_bool(gold_value) == to_bool(pred_value) else 0.0

    except Exception as e:
        logger.error(f"Error comparing {field_name}: {e}")
        return 0.0


# =============================================================================
# Convenience Functions
# =============================================================================

def intent_validation_metric(example, pred, trace=None) -> float:
    """Semantic metric for intent validation."""
    return semantic_boolean_match(example, pred, 'intent_satisfied', trace)


def completeness_metric(example, pred, trace=None) -> float:
    """Semantic metric for completeness validation."""
    return semantic_boolean_match(example, pred, 'is_complete', trace)


def correctness_metric(example, pred, trace=None) -> float:
    """Semantic metric for correctness validation."""
    return semantic_boolean_match(example, pred, 'is_correct', trace)
