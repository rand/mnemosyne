#!/usr/bin/env python3
"""Fast semantic evaluation metrics using holistic LLM-as-a-Judge.

Optimized version of semantic_metrics.py that evaluates entire requirement
sets at once rather than pairwise comparisons, reducing API calls by ~50-100x.

Maintains principled semantic evaluation while achieving practical performance
for optimization loops.
"""

import dspy
from typing import List
import logging

logger = logging.getLogger(__name__)


# =============================================================================
# Holistic Requirement Set Evaluator
# =============================================================================

class RequirementSetEvaluator(dspy.Signature):
    """Evaluate how well predicted requirements cover gold requirements.

    Compare complete requirement sets holistically and provide an F1-like
    score that captures:
    - Coverage: Do predictions capture all gold requirements?
    - Precision: Are predictions focused on gold requirements (not extra)?
    - Semantic equivalence: Do predictions match intent despite wording differences?

    Scoring guidelines:
    - 1.0: Perfect coverage, all gold requirements captured semantically
    - 0.8-0.9: Most requirements covered, minor gaps or extra details
    - 0.6-0.7: Core requirements covered, some missing or imprecise
    - 0.4-0.5: Partial coverage, significant gaps
    - 0.0-0.3: Poor coverage, mostly unrelated

    Output an F1-like score from 0.0 to 1.0.
    """

    gold_requirements = dspy.InputField(
        desc="List of reference requirements from training data (JSON array of strings)"
    )
    predicted_requirements = dspy.InputField(
        desc="List of requirements generated by the model (JSON array of strings)"
    )

    reasoning = dspy.OutputField(
        prefix="Reasoning: Let's analyze coverage step by step:",
        desc="Detailed analysis of coverage, precision, and semantic equivalence"
    )
    coverage_assessment = dspy.OutputField(
        desc="Summary of which gold requirements are covered and which are missing"
    )
    f1_score = dspy.OutputField(
        desc="F1-like score from 0.0 to 1.0 representing overall quality"
    )


class FastSemanticRequirementEvaluator(dspy.Module):
    """Fast DSPy module for holistic requirement set evaluation."""

    def __init__(self):
        super().__init__()
        self.evaluator = dspy.ChainOfThought(RequirementSetEvaluator)

    def forward(self, gold_reqs: List[str], pred_reqs: List[str]) -> float:
        """Evaluate predicted requirements against gold standard.

        Args:
            gold_reqs: Reference requirements from training data
            pred_reqs: Requirements generated by model

        Returns:
            F1-like score from 0.0 to 1.0
        """
        import json

        # Format as JSON arrays for clear structure
        gold_json = json.dumps(gold_reqs)
        pred_json = json.dumps(pred_reqs)

        result = self.evaluator(
            gold_requirements=gold_json,
            predicted_requirements=pred_json
        )

        try:
            # Parse score from string output, stripping whitespace and trailing characters
            score_str = str(result.f1_score).strip()
            # Extract numeric portion (handle cases like "0.82\n]]" or "0.82]]")
            import re
            match = re.search(r'(\d+\.?\d*)', score_str)
            if match:
                score = float(match.group(1))
            else:
                score = float(score_str)  # Try direct conversion

            # Clamp to valid range
            score = max(0.0, min(1.0, score))

            logger.debug(
                f"Fast semantic F1: {score:.3f} "
                f"({len(gold_reqs)} gold, {len(pred_reqs)} pred)"
            )

            return score
        except (ValueError, AttributeError) as e:
            logger.warning(f"Failed to parse F1 score from '{result.f1_score}': {e}")
            # Fallback: analyze coverage_assessment text
            assessment = str(result.coverage_assessment).lower()
            if "all" in assessment and "covered" in assessment:
                return 0.9
            elif "most" in assessment and "covered" in assessment:
                return 0.7
            elif "partial" in assessment or "some" in assessment:
                return 0.5
            else:
                return 0.2


# =============================================================================
# Fast Semantic F1 Metric
# =============================================================================

def semantic_requirement_f1(
    example: dspy.Example,
    pred: dspy.Prediction,
    trace=None,
    threshold: float = 0.7  # Unused but kept for API compatibility
) -> float:
    """Compute semantic F1 score for requirement extraction (fast version).

    Uses holistic LLM-as-a-Judge evaluation of entire requirement sets,
    replacing O(n*m) pairwise comparisons with a single API call.

    Args:
        example: Training example with gold requirements
        pred: Model prediction with predicted requirements
        trace: Optional DSPy trace (unused)
        threshold: Unused (kept for backward compatibility)

    Returns:
        F1 score from 0.0 to 1.0
    """
    try:
        gold_reqs = list(example.requirements)
        pred_reqs = list(pred.requirements) if hasattr(pred, 'requirements') else []

        if not pred_reqs:
            return 0.0

        if not gold_reqs:
            return 0.0

        # Holistic evaluation in single API call
        evaluator = FastSemanticRequirementEvaluator()
        score = evaluator(gold_reqs, pred_reqs)

        return score

    except Exception as e:
        logger.error(f"Error computing fast semantic F1: {e}")
        return 0.0


# =============================================================================
# Boolean Validation Metrics
# =============================================================================

def semantic_boolean_match(
    example: dspy.Example,
    pred: dspy.Prediction,
    field_name: str,
    trace=None
) -> float:
    """Evaluate binary field match with semantic parsing.

    Handles various boolean representations (True/False, "true"/"false",
    "yes"/"no") and returns 1.0 for match, 0.0 for mismatch.

    Args:
        example: Training example with gold boolean value
        pred: Model prediction with predicted boolean
        field_name: Name of the boolean field to compare
        trace: Optional DSPy trace (unused)

    Returns:
        1.0 if match, 0.0 if mismatch
    """
    try:
        gold_value = getattr(example, field_name, None)
        pred_value = getattr(pred, field_name, None)

        if gold_value is None or pred_value is None:
            return 0.0

        # Normalize to boolean
        def to_bool(val):
            if isinstance(val, bool):
                return val
            if isinstance(val, str):
                return val.strip().lower() in ['true', 'yes', '1']
            return bool(val)

        return 1.0 if to_bool(gold_value) == to_bool(pred_value) else 0.0

    except Exception as e:
        logger.error(f"Error comparing {field_name}: {e}")
        return 0.0


# =============================================================================
# Convenience Functions
# =============================================================================

def intent_validation_metric(example, pred, trace=None) -> float:
    """Semantic metric for intent validation."""
    return semantic_boolean_match(example, pred, 'intent_satisfied', trace)


def completeness_metric(example, pred, trace=None) -> float:
    """Semantic metric for completeness validation."""
    return semantic_boolean_match(example, pred, 'is_complete', trace)


def correctness_metric(example, pred, trace=None) -> float:
    """Semantic metric for correctness validation."""
    return semantic_boolean_match(example, pred, 'is_correct', trace)
